
<html>
	<head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1XTFM65VS2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1XTFM65VS2');
</script>
	<meta charset=“UTF-8”>

		<link href="https://fonts.googleapis.com/css?family=Montserrat:300,400,500,600" rel="stylesheet">
		<link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Open+Sans:400,bold,900,600">
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/css/bootstrap.min.css">
		  <!-- Custom styles for this template -->
 		 <link href="css/custom.css" rel="stylesheet">

	  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>
 	 <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/js/bootstrap.min.js"></script>
		<link rel="icon" href="img/lightcommands.png">

		<title>Light Commands</title>
	</head>
	<body>
		<div class = "global_container">
			<div class = "section"  >
<div class = "language"  >

 <a href="#" class="myButton_l"><img src = "img/usa-flag.png" width="20px"></img> US</a>  <a href="index_jp.html" class="myButton_l"><img src = "img/japan-flag.png" width="20px"> </img> JP</a>

</div>
				<img class="scale_img_85" src = "img/banner.png" >


				<h1>Laser-Based Audio Injection on Voice-Controllable Systems </h1>


				<div class ="boxed">
					<p> Light Commands is a vulnerability of MEMS microphones that allows attackers to remotely inject inaudible and invisible commands into
						voice assistants, such as Google assistant, Amazon Alexa, Facebook Portal, and Apple Siri using light.
					 </p>
						<p> In our <a href="20191104-Light-Commands.pdf">paper</a> we demonstrate this effect, successfully using light to inject malicious commands into several voice controlled
						devices such as smart speakers, tablets, and phones across large distances and through glass windows. </p>


						<img class="scale_img" src = "img/setup_tower.png"></img>



	<p class="my-4">The implications of injecting unauthorized voice commands vary in severity based on the type of commands that can be executed through voice.
		As an example, in our paper we show how an attacker can use light-injected voice commands to unlock the
victim's smart-lock protected home doors, or even locate, unlock and start various vehicles. </p>


						<a href="20191104-Light-Commands.pdf" class="myButton">Read the Paper</a>



						<a href="#bibtex"  class=" myButton " data-toggle="collapse" role="button">
					Cite <i class="fa fa-quote-right" aria-hidden="true"></i></a>
						<div id="bibtex" style="margin-top: 1.5em;" class="collapse" align="left">
							<pre style="white-space: pre">
@inproceedings{Sugawara2020LightCommands,
    author = {Sugawara, Takeshi and Cyr, Benjamin and Rampazzi, Sara and Genkin, Daniel and Fu, Kevin},
    title = {Light Commands: Laser-Based Audio Injection on Voice-Controllable Systems},
    year = {2019}
}
							</pre>




				</div>

</div>
<h1></h1>


					<h1> See Light Commands in Action </h1>
					<div style="margin-top=3em;">
						 <iframe width="390" height="250" src="https://www.youtube.com/embed/ORji7Tz5GiI?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
					<iframe width="390" height="250" src="https://www.youtube.com/embed/ihRAwc24nXw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
					<br>
					<iframe width="390" height="250" src="https://www.youtube.com/embed/iK2PtdQs77c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
					<iframe width="390" height="250" src="https://www.youtube.com/embed/EtzP-mCwNAs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
					<br>
                                        <iframe width="390" height="250" src="https://www.youtube-nocookie.com/embed/ozIKwGt38LQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

					</div>
					<br>


						<h1> Team </h1>
					<div class ="boxed" style="margin-top=3em;">
						<p>
							Light Commands were discovered by the following team of academic researchers:
						</p>
						<ul>
					<li><strong><a href="https://www.sugawara-lab.jp/member/sugawara_e">Takeshi Sugawara</a></strong> at <a href="https://www.uec.ac.jp/eng/">The University of Electro-Communications (Tokyo)</a></li>
								<li><a href="https://benjamin-cyr.com/"><strong>Benjamin Cyr</strong></a> at <a href="https://www.cse.umich.edu">University of Michigan</a></li>
				            	<li><a href="https://sararampazzi.com"><strong>Sara Rampazzi</strong></a> at <a href="https://www.cse.umich.edu">University of Michigan</a></li>
							<li><a href="https://web.eecs.umich.edu/~genkin/"><strong>Daniel Genkin</strong></a> at <a href="https://www.cse.umich.edu">University of Michigan</a></li>
							<li><a href="https://web.eecs.umich.edu/~kevinfu/"><strong>Kevin Fu</strong></a> at <a href="https://www.cse.umich.edu">University of Michigan</a></li>

						</ul>
<p class="text-center"><strong>Contact us at <a href="mailto:LightCommandsTeam@gmail.com">LightCommandsTeam@gmail.com</a></strong></p>

 <div class="text-center mt-auto" style="width:100%;margin-top: 1.5em; margin-bottom: 1.5em; display:flex;justify-content:space-around;align-items:center; flex-wrap: wrap;">
      <div>
      <a href="https://www.cse.umich.edu">
        <img width = "400px" src="img/umich_logo.png" alt="University of Michigan logo" class="img-fluid" />
	<div style = "height:10px"></div>
      </a>
      </div>
      <a href="https://www.uec.ac.jp/eng/">
        <img width="165px" src="img/uec.png" alt="UEC logo" class="img-fluid" />
</a>
      </div>


				</div>
				<h1>Q&A</h1>

					<ul class="question">
						<p>
						<li class="question">
							<b> How do Light Commands work?</b>

								<p class="answer">
									By shining the laser through the window at microphones inside smart speakers, tablets, or phones, a far away attacker can remotely send inaudible and potentially invisible commands which are then acted upon by Alexa, Portal, Google assistant or Siri.


</p>
								<p class="answer">
								Making things worse, once an attacker has gained control over a voice assistant, the attacker can use it to break other systems. For example, the attacker can:
</p>
							<ul class="answer">

<li class="answer">    Control smart home switches </li>
<li class="answer">    Open smart garage doors </li>
<li class="answer">    Make online purchases</li>
<li class="answer">    Remotely unlock and start certain vehicles</li>
<li class="answer">    Open smart locks by stealthily brute forcing the user's PIN number.</li>
</ul>
					</li>

						</p>
						<p>
						<li class="question">
							<b>But why does this happen?</b>

							<p class="answer">
							Microphones convert sound into electrical signals. The main discovery behind light commands is that in addition to sound, microphones also react to light aimed directly at them. Thus, by modulating an electrical signal in the intensity of a light beam, attackers can trick microphones into producing electrical signals as if they are receiving genuine audio.
							</p>
							<img class="scale_img" src = "img/waves.png"></img>



					</li>
						</p>

						<p>
						<li class="question">
							<b> Ok, but what do voice assistants have to do with this?</b>

							<p class="answer">
							Voice assistants inherently rely on voice to interact with the user. By shining a laser on their microphones, an attacker can effectively hijack the voice assistant and
							send inaudible commands to the Alexa, Siri, Portal, or Google Assistant.
							</p>
							<img class="scale_img_50" src ="img/setup_enclosure.png"></img>



					</li>
						</p>

												<p>
						<li class="question">
							<b>
							What is the range of Light Commands?
							</b>

								<p class="answer">
									Light can easily travel long distances, limiting the attacker only in the ability to focus and aim the laser beam. We have demonstrated the attack in a 110 meter hallway, which is the longest hallway available to us at the time of writing.
								</p>
								<img class="scale_img" src ="img/setup_corridor.png"></img>

					    </li>
						</p>
						<p>
						<li class="question">
							<b>
							But how can I aim the laser accurately, and at such distances?
							</b>

								<p class="answer">
									Careful aiming and laser focusing is indeed required for light commands to work. To focus the laser across large distances one can use a commercially available telephoto lens. Aiming can be done using a geared tripod head, which greatly increases accuracy. An attacker can use a telescope or binocular in order to see the device's microphone ports at large distances.
								</p>
								<img class="scale_img" src ="img/aiming.png"></img>

					    </li>
						</p>


						<p>
						<li class="question">
							<b> Which devices are susceptible to Light Commands?</b>

								<p class="answer">
									In our experiments, we test our attack on the most popular voice recognition systems, namely Amazon Alexa, Apple Siri, Facebook Portal, and Google Assistant.
									We benchmark multiple devices such as smart speakers, phones, and tablets as well as third-party devices with built-in speech recognition.
									</p>
							<table class="table table-condensed">
            <tbody><tr>
              <td><b>Device</b></td>
              <td><b>Voice Recognition<br> System</b></td>
              <td><b>Minimun Laser Power<br> at 30 cm [mW]</b></td>
		<td><b>Max Distance <br> at 60 mW [m]*</b></td>
		<td><b>Max Distance <br> at 5 mW [m]**</b></td>
            </tr>
            <tr>
              <td>Google Home</td>
              <td>Google Assistant</td>
              <td>0.5</td>
		<td>50+</td>
		<td>110+</td>
            </tr>
            <tr>
              <td>Google Home mini</td>
              <td>Google Assistant</td>
              <td>16</td>
		<td>20</td>
		<td>-</td>
            </tr>
            <tr>
              <td>Google NEST Cam IQ</td>
              <td>Google Assistant</td>
              <td>9</td>
		<td>50+</td>
		<td>-</td>
            </tr>
			<tr>
              <td>Echo Plus 1st Generation</td>
              <td>Amazon Alexa</td>
              <td>2.4</td>
		<td>50+</td>
		<td>110+</td>
            </tr>
            <tr>
              <td>Echo Plus 2nd Generation</td>
              <td>Amazon Alexa</td>
              <td>2.9</td>
		<td>50+</td>
		<td>50</td>
            </tr>
            <tr>
              <td>Echo</td>
              <td>Amazon Alexa</td>
              <td>25</td>
		<td>50+</td>
		<td>-</td>
            </tr>
            <tr>
              <td>Echo Dot 2nd Generation</td>
              <td>Amazon Alexa</td>
              <td>7</td>
		<td>50+</td>
		<td>-</td>
            </tr>
            <tr>
             	<td>Echo Dot 3rd Generation</td>
              <td>Amazon Alexa</td>
              <td>9</td>
		<td>50+</td>
		<td>-</td>
            </tr>
            <tr>
              <td>Echo Show 5</td>
              <td>Amazon Alexa</td>
              <td>17</td>
		<td>50+</td>
		<td>-</td>
            </tr>
            <tr>
              <td>Echo Spot</td>
              <td>Amazon Alexa</td>
              <td>29</td>
		<td>50+</td>
		<td>-</td>
            </tr>
			<tr>
              <td>Facebook Portal Mini</td>
              <td>Alexa + Portal</td>
              <td>18</td>
		<td>5</td>
		<td>-</td>
            </tr>
            <tr>
              <td>Fire Cube TV</td>
              <td>Amazon Alexa</td>
              <td>13</td>
		<td>20</td>
		<td>-</td>
            </tr>
            <tr>
              <td>EchoBee 4</td>
              <td>Amazon Alexa</td>
              <td>1.7</td>
		<td>50+</td>
		<td>70</td>
            </tr>
            <tr>
              <td>iPhone XR</td>
              <td>Siri</td>
              <td>21</td>
		<td>10</td>
		<td>-</td>
            </tr>
            <tr>
             <td>iPad 6th Gen</td>
              <td>Siri</td>
              <td>27</td>
		<td>20</td>
		<td>-</td>
            </tr>
            <tr>
              <td>Samsung Galaxy S9</td>
              <td>Google Assistant</td>
              <td>60</td>
		<td>5</td>
		<td>-</td>
            </tr>
            <tr>
              <td>Google Pixel 2</td>
              <td>Google Assistant</td>
              <td>46</td>
		<td>5</td>
		<td>-</td>
            </tr>
          </tbody></table>
								<p class="answer">
									While we do not claim that our list of tested devices is exhaustive, we do argue that it does provide some intuition about the vulnerability of popular voice recognition systems to Light Commands. <br> Note: <br>* Limited to a 50 m long corridor. <br> ** Limited to a 110 m long corridor.



								</p>

					</li>
						</p>


						<p>
						<li class="question">
							<b>
							Can speaker recognition protect me from Light Commands?
							</b>

								<p class="answer">
								At the time of writing, speaker recognition is off by default for smart speakers and is only enabled by default for devices like phones and tablets. Thus, Light Commands can be used on these smart speakers without imitating the owner's voice. Moreover, even if enabled, speaker recognition only verifies that the wake-up words (e.g., "Ok Google" or "Alexa") are said in the owner's voice, and not the rest of the command. This means that one "OK Google" or "Alexa" spoken by the owner can be used to compromise all the commands. Finally, as we show in our work, speaker recognition for wake-up words is often weak and can be sometimes bypassed by an attacker using online text-to-speech synthesis tools for imitating the owner's voice.

								</p>

					    </li>
						</p>
						<p>
						<li class="question">
							<b>
							Do Light Commands require special equipment? How can I build such a setup?
							</b>

								<p class="answer">
									The Light Commands attack can be mounted using a simple laser pointer (<a href="https://www.amazon.com/dp/B07PM1MF74">$13.99</a>, <a href="https://www.amazon.com/dp/B07WD723Z7">$16.99</a>, and <a href="https://www.amazon.com/gp/product/B07TZ4Q55Q">$17.99</a> on Amazon), a laser driver (<a href=https://www.teamwavelength.com/product/ld5cha-5-a-30-v-laser-diode-driver/">Wavelength Electronics LD5CHA</a>, $339), and a sound amplifier (<a href="https://www.amazon.com/gp/product/B01MS22YWV">Neoteck NTK059</a>, $27.99 on Amazon). A telephoto lens (<a href="https://www.amazon.com/dp/B001GKPOJK">Opteka 650-1300mm</a>, $199.95 on Amazon) can be used to focus the laser for long range attacks.
								</p>
								<img class="scale_img_50" src = "img/cheap_setup.png"></img>

					    </li>
						</p>
						<p>
						<li class="question">
							<b>
							How vulnerable are other voice controllable systems?
							</b>

								<p class="answer">
									While our paper focuses on Alexa, Siri, Portal, and Google Assistant, the basic vulnerability exploited by Light Commands stems from design issues in MEMS microphones. As such, any system that uses MEMS microphones and acts on this data without additional user confirmation might be vulnerable.
								</p>


					    </li>
						</p>
						<p>
						<li class="question">
							<b>
							How can I detect if someone used Light Commands against me?
							</b>

								<p class="answer">
									While command injection via light makes no sound, an attentive user can notice the attacker's light beam reflected on the target device. Alternatively, one can attempt to monitor the device's verbal response and light pattern changes, both of which serve as command confirmation.


								</p>


					    </li>
						</p>
						<p>
						<li class="question">
							<b>
							Have Light Commands been abused in the wild?
							</b>

								<p class="answer">
								So far we have not seen any indications that this attack have been maliciously exploited.
								</p>


					    </li>
						</p>
					<p>
						<li class="question">
							<b>
							Does the effect depend on laser color or wavelength?
							</b>

								<p class="answer">
									During our experiments, we note the effects are generally independent from color and wavelength. Although blue and red lights are on the other edges
									in the visible spectrum, the levels of injected audio signal are in the same range and the shapes of the frequency-response curves are also similar.
									</p>
								<img class="scale_img_50" src = "img/light-spectrum.png"></img>

					    </li>
						</p>
						<p>
						<li class="question">
							<b>
							Do I have to use a laser? What about other light sources?
							</b>

								<p class="answer">
									In principle any bright enough light can be used to mount our attack. For example the <a href="http://www.acebeam.com/w30">Acebeam W30</a> laser-excited flashlight can be used as an alternative to a laser diode.
								</p>
								<img class="scale_img_50" src = "img/flashlight.png"></img>

					    </li>
						</p>

						<p>
						<li class="question">
							<b>
							Is it possible mitigate this issue?
							</b>

								<p class="answer">
									An additional layer of authentication can be effective at somewhat mitigating the attack.
									Alternatively, in case the attacker cannot eavesdrop on the device's response, having the device
									ask the user a simple randomized question before command execution can be an effective way at preventing the attacker
									from obtaining successful command execution.
									</p>
									<p class="answer">
									Manufacturers can also attempt to use sensor fusion techniques, such as acquire audio from multiple microphones.
									When the attacker uses a single laser, only a single microphone receives a signal while the others receive
										nothing. Thus, manufacturers can attempt to detect such anomalies, ignoring the injected commands.
									</p>
									<p class="answer">

										Another approach consists in reducing the amount of light reaching the microphone's
										diaphragm using a barrier that physically blocks straight light beams for eliminating the line of sight to the diaphragm,
										or implement a non-transparent cover on top of the microphone hole for attenuating the amount of light
										hitting the microphone. However, we note that such physical barriers are only effective to a certain point,
										as an attacker can always increase the laser power in an attempt to compensate for the
										cover-induced attenuation or for burning through the barriers, creating a new light path.





								</p>

					    </li>
						</p>
						<p>
						<li class="question">
							<b>
							Is the laser beam used in the attack safe?</b>

								<p class="answer">
									Laser radiation requires special controls for safety, as high-powered lasers might cause hazards of fire, eye	damage, and skin damage.
									In this work, we consider low power <a href="https://www.lasersafetyfacts.com/3R/"> Class 3R </a> and <a href="https://www.lasersafetyfacts.com/3B/"> Class 3B </a> lasers. Class 3R
									devices (such as common laser pointers)	emit under 5 mW of optical power, and are considered safe to human eyes for brief exposure durations.

							<p class="answer">

									Class 3B Systems emit between 5 and 500 mW, and might cause eye injury even from short beam exposure durations.
									We urge that researchers receive formal laser safety training and approval of experimental designs before attempting reproduction of our work.

								</p>

					    </li>
						</p>
						<p>
						<li class="question">
							<b>
							Why is it called Light Commands?</b>

								<p class="answer">
								Our work is called Light Commands as we exploit light (as opposed to sound) as the primary medium to carry the command to the microphone.
								</p>

					    </li>
						</p>
					<p>
						<li class="question">
							<b>
							Can I use the logo?
							</b>

								<p class="answer">
									 The logo is free to use, rights waived via <a href="https://creativecommons.org/publicdomain/zero/1.0/">CC0</a>. Logos are designed by <a href="https://www.linkedin.com/in/emilio-pimentel-96253330/">Emilio Pimentel</a>.
								</p>
								<table class="table">
							   <thead>
								 <tr>
								   <th>Logo</th>
								   <th>Logo with side text</th>
								   <th>Logo with bottom text</th>
								 </tr>
							   </thead>
							   <tbody>
								 <tr>
								   <td><a href="img/lightcommands.png"><i class="fa fa-download" aria-hidden="true"></i> PNG</a> /
									   <a href="img/lightcommands.svg"><i class="fa fa-download" aria-hidden="true"></i> SVG</a></td>

								   <td><a href="img/lightcommands_side_text.png"><i class="fa fa-download" aria-hidden="true"></i> PNG</a> /
									   <a href="img/lightcommands_side_text.svg"><i class="fa fa-download" aria-hidden="true"></i> SVG</a></td>

								   <td><a href="img/lightcommands_bottom_text.png"><i class="fa fa-download" aria-hidden="true"></i> PNG</a> /
									   <a href="img/lightcommands_bottom_text.svg"><i class="fa fa-download" aria-hidden="true"></i> SVG</a></td>
								 </tr>
								<tr><td/><td/><td/></tr>
							   </tbody>
							 </table>
					    </li>
						</p>



				</ul>

				<h2>Acknowledgments</h2>
				<p>
This research was funded by JSPS KAKENHI Grant Number JP18K18047 and JP18KK0312, by the Defense Advanced Research Projects Agency (DARPA) under contract FA8750-19-C-0531, gifts from Intel, AMD, and Analog Devices, an award from MCity at University of Michigan, and by the National Science Foundation under grant CNS-1330142.
				</p>

	</body>
</html>
